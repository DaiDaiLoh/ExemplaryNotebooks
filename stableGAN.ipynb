{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1. Imports</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>2. Global definitions & helpers</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_DIMS = 128 #number of random values to put into our GAN\n",
    "INT_CHANNELS = 256 #number of channels in the intermediate layers of the generator and discriminator, i.e. how powerful the GAN is\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(mnist_trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "def show(img):\n",
    "    assert(len(img.size()) == 3)\n",
    "    plt.imshow(img.squeeze().clamp(-1, 1), cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "for img, label in train_loader:\n",
    "    #show example image:\n",
    "    show(img[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>3. Network architecture </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trivial risidual block: apply convolution twice, then add the original input projected to the output size\n",
    "#tl;dr \"do some computation, then add the original input to the result to make sure our gradients are somewhat stable and we train without vanishing gradients\"\n",
    "class ResBlock(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, skip_last_relu=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        #without bias & with kernel size 1, this is effectively just \"projecting\" the input to the output size without doing much. this can also scale that residual just to zero if we want\n",
    "        self.conv_res = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0, stride=1, bias=False)\n",
    "\n",
    "        self.conv_1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv_2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1)\n",
    "        self.conv_3 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        #leaky relu = better gradient flow / prevention of \"dead\" neurons that do not change anymore\n",
    "        self.relu = torch.nn.LeakyReLU(negative_slope=0.01) #slope 0.01 basically means: \"max(x, 0.01 * x)\", i.e. if we are < 0, then we do not clamp to 0, but to 0.01 * x (e.g. -5 gets clamped to -0.05)\n",
    "        self.relu_last = torch.nn.LeakyReLU(negative_slope=0.01)\n",
    "        if skip_last_relu:\n",
    "            #our target values we want to generate are between [-1, 1] - applying (leaky) relu would basically (soft) clamp us to [0, inf);\n",
    "            #hence, in those cases, just do nothing. that's faster than some ugly \"if\"-branch in the forward method\n",
    "            self.relu_last = torch.nn.Identity()\n",
    "            #also don't apply norm there at the end\n",
    "            self.norm_1 = torch.nn.Identity()\n",
    "            self.norm_2 = torch.nn.Identity()\n",
    "        else:\n",
    "            #normalisation layers usually help with training stability, especially in deeper networks\n",
    "            self.norm_1 = torch.nn.GroupNorm(num_groups=16, num_channels=out_channels)\n",
    "            self.norm_2 = torch.nn.GroupNorm(num_groups=16, num_channels=out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_res = self.conv_res(x)\n",
    "        \n",
    "        x = self.norm_1(self.relu(self.conv_1(x)))\n",
    "        x = self.norm_2(self.relu(self.conv_2(x)))\n",
    "        x = self.relu_last(self.conv_3(x))\n",
    "        x = x + x_res\n",
    "        return x\n",
    "\n",
    "#simple generator: take random noise, process it a bit (\"let the generator decide on what it should generate\"), then upscale/residual block it until we reach target size\n",
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        #first part: generate a 7x7 base image from random noise\n",
    "        self.lin_res = torch.nn.Linear(Z_DIMS + 8, 16*7*7, bias=False)\n",
    "        self.lin_1 = torch.nn.Linear(Z_DIMS + 8, 128)\n",
    "        self.lin_2 = torch.nn.Linear(128, 128)\n",
    "        self.lin_3 = torch.nn.Linear(128, 16*7*7)\n",
    "\n",
    "        #second part: alternate between residual blocks and upscaling until we reach target size\n",
    "        self.res_1 = ResBlock(16, INT_CHANNELS)\n",
    "        self.res_2 = ResBlock(INT_CHANNELS, INT_CHANNELS)\n",
    "        self.res_3 = ResBlock(INT_CHANNELS, INT_CHANNELS)\n",
    "        self.res_4 = ResBlock(INT_CHANNELS, 1, skip_last_relu=True) #don't apply a relu at the end: we output images with values between [-1, 1], applying a relu would clamp us to [0, inf)\n",
    "\n",
    "        #residual: bring in the noise again at different parts of the network to make sure we do not lose information on what we generate\n",
    "        self.residual_conv = torch.nn.Conv2d(16, INT_CHANNELS, kernel_size=1, padding=0, stride=1, bias=False)\n",
    "\n",
    "        #we use upsclaing, because transposed convolution layers create some ugly artifacts: https://distill.pub/2016/deconv-checkerboard/\n",
    "        #just dumb upscaling and then applying a convolution layers does the same trick, just without artifacts\n",
    "        self.upscale = torch.nn.Upsample(scale_factor=2.0, mode='bilinear')\n",
    "\n",
    "        self.relu = torch.nn.LeakyReLU(negative_slope=0.01)\n",
    "\n",
    "        #we put labels into our generator/discriminator to make it a conditional GAN, i.e. producing the numbers we want\n",
    "        #for that, we use an embedding layer to project the labels into some higher space; just putting \"8\" in there is more difficult\n",
    "        #for a network than putting a vector in there that represents \"8\" with multiple dimensions\n",
    "        self.embedding = torch.nn.Embedding(10, 8)\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        #embed the labels into some higher dimentional space that's easier for our NN, then concatenate the labels to the input noise\n",
    "        labels = self.embedding(labels)\n",
    "        x = torch.cat([x, labels], dim=1)\n",
    "        #take input vector, then project it to [b x 1 x 7 x 7]\n",
    "        x_res = self.lin_res(x).view(x.size()[0], -1, 7, 7)\n",
    "        x = self.relu(self.lin_1(x))\n",
    "        x = self.relu(self.lin_2(x))\n",
    "        x = self.relu(self.lin_3(x))\n",
    "        #re-shape so we have a [b x 16 x 7 x 7] tensor (\"tiny\" image with some channels)\n",
    "        x = x.view(x.size()[0], 16, 7, 7) + x_res\n",
    "        #second part: alternate between residual blocks and upscaling until we reach target size\n",
    "        x = self.res_1(x)\n",
    "        x = self.upscale(x) + self.residual_conv(self.upscale(x_res))\n",
    "        #residual: bring in the noise again\n",
    "        \n",
    "        # now at [b x 64 x 14 x 14]\n",
    "        x = self.res_2(x)\n",
    "        x = self.upscale(x) + self.residual_conv(self.upscale(self.upscale(x_res)))\n",
    "        # now at [b x 64 x 28 x 28]\n",
    "        x = self.res_3(x)\n",
    "        x = self.res_4(x)\n",
    "        # now at [b x 1 x 28 x 28]\n",
    "        return x\n",
    "\n",
    "#simple discriminator: take an image, process it with a CNN, then output a value in roughly [-1, 1]\n",
    "#basically, the generator architecture in reverse: interleave residual blocks and downscaling, then process with linear layers when information is dense enough\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        #first part: process the image with our convolutional residual blocks\n",
    "        self.res_1 = ResBlock(1+8, INT_CHANNELS) #process image(1) and label embedding(8)\n",
    "        self.res_2 = ResBlock(INT_CHANNELS, INT_CHANNELS)\n",
    "        self.res_3 = ResBlock(INT_CHANNELS, 16) #no activation at the end - then we can get values in [-1, 1]\n",
    "        self.downscale = torch.nn.Upsample(scale_factor=0.5, mode='bilinear') #same reason as for the upscaling: if we use convolutional layers, we get ugly artifacts. this is basically just avg pooling now\n",
    "        #second part: process the image with linear layers\n",
    "        self.lin_1 = torch.nn.Linear(16*7*7, 128)\n",
    "        self.lin_2 = torch.nn.Linear(128, 128)\n",
    "        self.lin_3 = torch.nn.Linear(128, 1)\n",
    "        #for leaky relu here, use a slope of 0.1, because that gives us a bit more stable gradients:\n",
    "        #the decision isn't as \"harsh\", hence the generator has an easier time to fix it's mistakes\n",
    "        #(easier to \"learn\" the correct direction, i.e. if generator outputs are still wrong, but less wrong) \n",
    "        self.relu = torch.nn.LeakyReLU(negative_slope=0.1)\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(10, 8)\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        labels = self.embedding(labels)\n",
    "        #simply concatenate the labels to every pixel of the image\n",
    "        labels = labels.view(labels.size()[0], 8, 1, 1).repeat(1, 1, 28, 28)\n",
    "        x = torch.cat([x, labels], dim=1)\n",
    "\n",
    "        #first part: process the image with our convolutional residual blocks\n",
    "        x = self.res_1(x)\n",
    "        x = self.downscale(x)\n",
    "        # now at [b x 64 x 14 x 14]\n",
    "        x = self.res_2(x)\n",
    "        x = self.downscale(x)\n",
    "        # now at [b x 64 x 7 x 7]\n",
    "        x = self.res_3(x)\n",
    "        # now at [b x 16 x 7 x 7]\n",
    "        x = x.view(x.size()[0], 16*7*7)\n",
    "        #second part: process the image with linear layers; could also skip this part, giving us basically a patch-wise discriminator as done by many papers!\n",
    "        x = self.relu(self.lin_1(x))\n",
    "        x = self.relu(self.lin_2(x))\n",
    "        x = self.lin_3(x) #no activation at the end - then we can get values in [-1, 1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>4. Training and inference</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the generator and discriminator\n",
    "generator = Generator().to(DEVICE)\n",
    "discriminator = Discriminator().to(DEVICE)\n",
    "\n",
    "print(\"The generator has \", sum([p.numel() for p in generator.parameters()]), \" parameters\")\n",
    "print(\"The discriminator has \", sum([p.numel() for p in discriminator.parameters()]), \" parameters\")\n",
    "\n",
    "gen_opt = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "disc_opt = torch.optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "hard_relu = torch.nn.ReLU()\n",
    "\n",
    "for epoch in range(0, 1000):\n",
    "    step = 0\n",
    "    loss_disc = 0\n",
    "    loss_gen = 0\n",
    "    for real, real_labels in train_loader:\n",
    "        #1. put stuff on the GPU\n",
    "        real = real.to(DEVICE)\n",
    "        real_labels = real_labels.to(DEVICE)\n",
    "        \n",
    "        noise = torch.rand(real.size()[0], Z_DIMS, device=DEVICE) * 2.0 - 1.0 #noise in [-1, 1] is most stable\n",
    "        fake_labels = torch.randint(0, 10, (real.size()[0],), device=DEVICE)\n",
    "        \n",
    "        #2. train the discriminator\n",
    "        if True:\n",
    "            discriminator.train(True)\n",
    "            generator.train(False)\n",
    "            disc_opt.zero_grad()\n",
    "            fake = generator(noise, fake_labels)\n",
    "            out_real = discriminator(real, real_labels)\n",
    "            out_fake = discriminator(fake.detach(), fake_labels)\n",
    "\n",
    "            #hinge loss: output -1 for fake, 1 for real images, but do not update anything if we are already correct enough (i.e. if a real is 1.5, we are already correct enough, don't update)\n",
    "            #this is the \"hinge\" part: if we are already correct enough, we do not update the weights\n",
    "            #aka the big \"magic\" to make GANs stable; traditional sigmoid on the output, then train to 1/0 is very unstable as the discriminator can easily \"overpower\" the generator \n",
    "            #--> what ever the generator outputs, it is so wrong that the discriminator can easily distinguish it and we have no gradients to update the generator\n",
    "\n",
    "            #if real image is mapped to > 1, this clamps it to 0, if fake image is mapped to < -1, this clamps it to 0 (\"don't update if we're good enough already\" aka \"wait for the generator to catch up\")\n",
    "            loss = hard_relu(1.0 - out_real).mean() + hard_relu(1.0 + out_fake).mean()\n",
    "            loss_disc += loss.item()\n",
    "            loss.backward()\n",
    "            disc_opt.step()\n",
    "        #3. train the generator\n",
    "        if True:\n",
    "            #important detail: we use the SAME noise for the generator because that'S what just updated\n",
    "            discriminator.train(False)\n",
    "            generator.train(True)\n",
    "            gen_opt.zero_grad()\n",
    "\n",
    "            fake = generator(noise, fake_labels)\n",
    "            out_fake = discriminator(fake, fake_labels)\n",
    "            #train the generator to \"fool\" the discriminator; if the discriminator is already fooled enough, don't update the generator\n",
    "            loss = hard_relu(1.0 - out_fake).mean()\n",
    "            loss_gen += loss.item()\n",
    "            loss.backward()\n",
    "            gen_opt.step()\n",
    "        \n",
    "        step += 1\n",
    "        if step == 1000: #don't wait for the whole dataset, we want to see stuff quickly...\n",
    "            break\n",
    "\n",
    "    print('*** DONE WITH EPOCH',epoch,' ***')\n",
    "    print(\"\\tAVG loss discriminator: \", loss_disc/step)\n",
    "    print(\"\\tAVG loss generator    : \", loss_gen /step)\n",
    "    #show some example images\n",
    "    with torch.no_grad():\n",
    "        generator.eval()\n",
    "        noise = torch.rand(16, Z_DIMS, device=DEVICE) * 2.0 - 1.0\n",
    "        fake = generator(noise, torch.randint(0, 10, (16,), device=DEVICE)).cpu()\n",
    "        #form of [16 x 1 x 28 x 28] - plot as 4x4 grid\n",
    "        out = torch.zeros(1, 4*28, 4*28)\n",
    "        #dumb loop is slow, but we don't care for just the output here\n",
    "        for x in range(0, 4):\n",
    "            for y in range(0, 4):\n",
    "                out[0, x*28:(x+1)*28, y*28:(y+1)*28] = fake[x*4+y, 0]\n",
    "        show(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
