{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>VQ-VAE</h1>\n",
    "Rough idea: Autoencode an input image, here demonstrated on [3 x 32 x 32] images, but express the latent code not as continous values,<br/>\n",
    "but as quantised values (i.e. indices in a codebook).<br/>\n",
    "Technically, this is pretty similar to applying K-means to the latent space, with the centroid being the \"codewords\"; we then simply round the codewords to the nearest centroid.<br/>\n",
    "The paper recommends using EMA to update the codebook (=move each codeword a bit towards the average of the elements that fall into that codeword)<br/>\n",
    "<br/>\n",
    "<font color=\"red\"><b>WARNING:</b></font> The results here are a bit blurry! VQVAE performs pretty bad on small images (CIFAR) when going from 32-by-32 to 8-by-8, i.e. with a compression rate of turning a 4-by-4 block into ONE token.<br/>\n",
    "For larger images, results look okay-ish (=less structure gets mashed together; if we have 256p images of a fish, then the scales don't have to look crisp for the image to be crisp when using a factor of four like here). This is just a demonstration! Toy around, throw one downscaling out (=do 16-by-16), use more codewords, etc., to make things look better!<br/>\n",
    "<br/>\n",
    "Applications for this include e.g. autoregressive probabilistic generation. We can also sharpen a VQ-VAE with an additional discriminator (\"VQGAN\"), i.e. reconstructing lost details by \"imagining\" something new. Works particularly great in high resolution images (not so much on e.g. CIFAR), as there are a lot of details that are not important; on CIFAR, every single pixel is needed for structure anyway; but e.g. on high-res images of fishes, VQGAN is great at learning e.g. the patterns of their scales instead of just outputting some \"averaged\" colour for the area<br/>\n",
    "<br/>\n",
    "Literature I recommend:<br/>\n",
    "1. VQ-VAE:  https://arxiv.org/abs/1711.00937<br/>\n",
    "2. VQGAN: https://arxiv.org/abs/2012.09841<br/>\n",
    "3. Improved VQGAN: https://arxiv.org/abs/2310.05400 (uses a Wasserstein discriminator and makes sure all codewords are used properly)<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "import platform\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "\n",
    "#nobody likes scientific rep...\n",
    "torch.set_printoptions(precision=6, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize(32), transforms.ToTensor()])\n",
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True, num_workers=0)\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.clamp(0.0, 1.0).numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "for images, labels in trainloader:\n",
    "    imshow(torchvision.utils.make_grid(images))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VQVAE_D = 64 #number of dimensions in quantised space, i.e. the number of dimensions in which we cluster (round in [b x VQVAE_D x W X H] in dimension 1 to the nearest centroid (i.e. codeword))\n",
    "# rule of thumb: VQVAE_D should be at least such that CODEWORDS <= 2**VQVAE_D to ensure that the quantised space is large enough to capture the variance in the data;\n",
    "# trivial example: if we use VQVAE_D = 1, then we can only cluster the data in one dimension, which doesn't work that well as networks are good at doing binary decisions, but not good at doing decisions with 512 different options\n",
    "# if we use e.g. 4 dimensions, then we can do 4 binary decisions with our relus, hence we have 16 different options - better than 1, but still not enough for 512 different options (=CODEWORDS)\n",
    "VQVAE_K = 512 #number of codewords in the codebook, i.e. the number of centroids in the quantised space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAE(nn.Module):\n",
    "    def __init__(self, c_in):\n",
    "        super(BaseAE, self).__init__()\n",
    "\n",
    "        CHANNELS = 256\n",
    "\n",
    "        self.relu = torch.nn.LeakyReLU() #we use leaky relu as activation function, as it also has a gradient for an input < 0\n",
    "        self.reduce = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.conv_res = torch.nn.Conv2d(in_channels=c_in, out_channels=VQVAE_D, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\n",
    "        #super basic encoder (convolution -> relu -> pooling -> repeat), to improve: replace conv2d with a full residual block and add some normalisation\n",
    "        #we don't use strided convolution to reduce the spatial dimensions, but instead use pooling, as this usually avoids checkerboard artefacts: https://distill.pub/2016/deconv-checkerboard/\n",
    "        self.encoder = nn.Sequential()\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=c_in, out_channels=CHANNELS, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        self.encoder.append(self.reduce)\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=CHANNELS, out_channels=CHANNELS, kernel_size=3, stride=1, padding=1))\n",
    "        self.encoder.append(self.relu)\n",
    "        self.encoder.append(self.reduce)\n",
    "        self.encoder.append(torch.nn.Conv2d(in_channels=CHANNELS, out_channels=VQVAE_D, kernel_size=3, stride=1, padding=1))\n",
    "        \n",
    "        #same for the decoder, but with upsampling instead of pooling. same as above, no transposed convolutions, but upsampling, as this usually avoids checkerboard artefacts\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=VQVAE_D, out_channels=CHANNELS, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu)\n",
    "        self.decoder.append(torch.nn.Upsample(scale_factor=2, mode='bilinear'))\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=CHANNELS, out_channels=CHANNELS, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu)\n",
    "        self.decoder.append(torch.nn.Upsample(scale_factor=2, mode='bilinear'))\n",
    "        self.decoder.append(torch.nn.Conv2d(in_channels=CHANNELS, out_channels=c_in, kernel_size=3, stride=1, padding=1))\n",
    "        self.decoder.append(self.relu) #we have [0, 1] for our data (not [-1, 1]), so we CAN use a relu; if we'd normalise data, this would kill of negative values (which we don't have)\n",
    "\n",
    "        #this is the values we round to, i.e. the \"centroids\" in the quantised space\n",
    "        #little trick to get better initialisation:\n",
    "        #   if we get values far away from any encoder outputs by chance, we have no gradient to these codewords\n",
    "        #   hence, rather set the codewords a bit \"closer\" around the centre, so we have a gradient for all codewords \n",
    "        self.codebook = torch.nn.Parameter((torch.rand(VQVAE_K, VQVAE_D) * 2.0 - 1.0) * 0.5, requires_grad=True)\n",
    "\n",
    "    def quantise(self, x):\n",
    "        #this is where the magic happens:\n",
    "        #    1. take input of size [b x W x H x VQVAE_D] and reshape to [b x W x H x VQVAE_D];\n",
    "        #       this means we have ALL the codewords in the last dimension now, so we can just merge together all other dimensions to easily compare them to the codewords\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        size_in = x.size()\n",
    "        x = x.view(-1, VQVAE_D)\n",
    "        #   2. compare [b*W*H x VQVAE_D] to [CODEWORDS x VQVAE_D]: by re-shaping to have [b*W*H x 1 x VQVAE_D] and [1 x CODEWORDS x VQVAE_D], the substraction will\n",
    "        #      give us a [b*W*H x CODEWORDS X VQVAE_D] tensor of all distance pairs between input elements (b*W*H) and the CODEWORDS-many codewords; just square, \n",
    "        #      then sum up the last dimension to get sum of the distance values for each dimension (=euclidean squared distance)\n",
    "        #      and then take the argmin to get the index of the closest codeword for each input element - these are our indices we want!\n",
    "        indices = (x[:,None] - self.codebook.view(1, VQVAE_K, VQVAE_D)).square().sum(dim=2).argmin(dim=1)\n",
    "        #   3. we take those indices we just looked up: these are the indices of the closest codeword for each input element, so we can now look up the actual \n",
    "        #      codeword to find the closest codeword to each input element aka the \"centroids\" we round to\n",
    "        x_rounded = self.codebook[indices]\n",
    "        \n",
    "        #   4. we calculate the loss for the commitment loss (i.e. does the encoder produce stuff from the codebook?) and the codebook loss (how close is the codebook to the encoder outputs?)\n",
    "        #      the value is exactly the same for both terms, but this way we can a) train encoder and codebook with different magnitudes and b) the encoder outputs and codebook outputs are\n",
    "        #      not just \"shrinked\" to one super tiny value: the loss we be almost zero if codebook and encoder outputs would be just downscaled by some tiny factor (latent space just shrinks together)\n",
    "        #      to avoid that, we use the detach() function, making sure that codebook and encoder outputs move individually and not just shrink together to something tiny\n",
    "        loss_commitment = (x - x_rounded.detach()).square().mean() #only have gradient for encoder \n",
    "        loss_codebook   = (x.detach() - x_rounded).square().mean()   #only have gradient for codebook\n",
    "\n",
    "        #   5. the actual rounding we do: we take the difference between the rounded and the unrounded value and add it to the unrounded value\n",
    "        #      the paper calls this \"straight through estimator\", as we just pass the gradient through the rounding operation (which is not differentiable)\n",
    "        #      what essentially happens here is that we just take our x and substract some float without any gradient, similar to writing \"x = x - 0.1\":\n",
    "        #      \"(x - x_rounded).detach()\" becomes just some number that we substract from x; this converts x to the same values as x_rounded, but the gradient is not passed through this operation\n",
    "        #      i.e. we round, but keep the (then a bit inexact) gradient \n",
    "        x = x - (x - x_rounded).detach() #change x to x_rounded, but keep the gradient from x\n",
    "\n",
    "        #   6. we reshape back to the original shape and return the values; we also return both losses and the indices of the closest codewords\n",
    "        x = x.view(size_in)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return x, loss_codebook, loss_commitment, indices.view(x.size()[0], x.size()[2], x.size()[3])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #encode, then quantise, then decode; pass the codebook loss and commitment loss through so we can use them for training\n",
    "        w, h = x.size()[2], x.size()[3]\n",
    "        x_res = self.conv_res(self.reduce(self.reduce(x))) #simple residual connection; makes optimisation more stable\n",
    "        x = self.encoder(x)\n",
    "        x = x + x_res\n",
    "        x, loss_codebook, loss_commitment, indices = self.quantise(x)\n",
    "        x = self.decoder(x)\n",
    "        return x, loss_codebook, loss_commitment, indices\n",
    "\n",
    "#pretty much standard for everything else - put data through, train, that's it\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = BaseAE(c_in=3).to(DEVICE)\n",
    "print(\"CONV NET HAS \", sum(p.numel() for p in net.parameters() if p.requires_grad), \" PARAMS\")\n",
    "optim = torch.optim.AdamW(net.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "for epoch in range(0, 100):\n",
    "    avg_losses = [0, 0, 0]\n",
    "    for data, labels in trainloader:\n",
    "        optim.zero_grad()\n",
    "        data = data.to(DEVICE)\n",
    "\n",
    "        outputs, loss_codebook, loss_commitment, indices = net(data)\n",
    "        #indices are the indices of the closest codewords for each input element;\n",
    "        #hence we have [8 x 8] int values in [0, 512) for each image in the batch\n",
    "        #if we just write them one after the other, as 64 values, we can e.g. train a transformer\n",
    "        #on them to learn to autoregressively generate these codewords to produce something we can decode into an image\n",
    "         \n",
    "        loss_reconstruction = (outputs - data).square().mean()\n",
    "        #loss is composed of reconstruction (=input should be output), codebook (=codebook should be close to encoder outputs) and commitment loss (encoder outputs should be close to the codebook values)\n",
    "        #instead of 0.25, you can also use 2.0 or something, does not matter too much\n",
    "        loss = loss_reconstruction + loss_codebook * 0.25 + loss_commitment\n",
    "        loss.backward()\n",
    "        avg_losses[0] += loss_reconstruction.item()\n",
    "        avg_losses[1] += loss_codebook.item()\n",
    "        avg_losses[2] += loss_commitment.item()\n",
    "        optim.step()\n",
    "\n",
    "    print(\"*** DONE WITH EPOCH \", epoch, \"***\")\n",
    "    print(\"\\tTrain loss: rec=\", avg_losses[0] / len(trainloader),\", code=\", avg_losses[1] / len(trainloader),\", comm=\", avg_losses[2] / len(trainloader))\n",
    "    #also run the test set through the network\n",
    "    avg_losses = [0, 0, 0]\n",
    "    step = 0\n",
    "    #track unique indices:\n",
    "    unique_indices = set()\n",
    "    for data, labels in testloader:\n",
    "        data = data.to(DEVICE)\n",
    "        outputs, loss_codebook, loss_commitment, indices = net(data)\n",
    "        loss_reconstruction = (outputs - data).square().mean()\n",
    "        avg_losses[0] += loss_reconstruction.item()\n",
    "        avg_losses[1] += loss_codebook.item()\n",
    "        avg_losses[2] += loss_commitment.item()\n",
    "\n",
    "        #count unique indices:\n",
    "        unique_indices.update(torch.unique(indices).tolist())\n",
    "        #make sure unique indices stay unique\n",
    "        unique_indices = set(unique_indices)\n",
    "        #show the first item from the testset\n",
    "        if step == 0:\n",
    "            imshow(torchvision.utils.make_grid(data.cpu()))\n",
    "            imshow(torchvision.utils.make_grid(outputs.cpu().detach()))\n",
    "            step += 1\n",
    "    print(\"\\tThe test set has seen \", len(unique_indices), \"/\",VQVAE_K,\" unique indices\") #should be somewhat close to VQVAE_K; else use some techniques to either re-initialise the codebook or regularise the encoder distribution\n",
    "    print(\"\\tTest loss: rec=\", avg_losses[0] / len(testloader),\", code=\", avg_losses[1] / len(trainloader),\", comm=\", avg_losses[2] / len(trainloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
